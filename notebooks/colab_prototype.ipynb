{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# C2S-Scale-Gemma Hybrid Model - Colab Prototype\n",
        "\n",
        "This notebook demonstrates the complete pipeline for training and evaluating the C2S-Scale-Gemma hybrid model, which combines:\n",
        "- **UHG-HGNN Encoder**: Hyperbolic Graph Neural Network for graph signal processing\n",
        "- **C2S-Scale-Gemma Text Encoder**: Large language model for text processing\n",
        "- **LoRA Adapters**: Parameter-efficient fine-tuning\n",
        "- **Contrastive Alignment**: InfoNCE loss with hard negative mining\n",
        "- **Late Fusion**: Combines graph and text representations\n",
        "\n",
        "## Overview\n",
        "\n",
        "The hybrid model processes single-cell transcriptomics data through two parallel encoders:\n",
        "1. **Graph Encoder**: Processes cell-cell interaction graphs using hyperbolic geometry\n",
        "2. **Text Encoder**: Processes cell descriptions using the Gemma language model\n",
        "3. **Fusion**: Combines representations through learned fusion heads\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's install the required dependencies and set up the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install uhg torch transformers accelerate peft datasets scikit-learn scanpy anndata umap-learn pynndescent mlflow omegaconf networkx pandas numpy tqdm pyyaml wandb python-dotenv\n",
        "\n",
        "# Install bitsandbytes for quantization (if available)\n",
        "try:\n",
        "    !pip install bitsandbytes\n",
        "except:\n",
        "    print(\"bitsandbytes not available on this platform\")\n",
        "\n",
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Download and Preparation\n",
        "\n",
        "Download the Cell2Sentence dataset from HuggingFace and prepare it for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Cell2Sentence dataset\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "# Load dataset from HuggingFace\n",
        "dataset = load_dataset(\"vandijklab/cell2sentence\")\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"Train size: {len(dataset['train'])}\")\n",
        "print(f\"Validation size: {len(dataset['validation'])}\")\n",
        "print(f\"Test size: {len(dataset['test'])}\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nSample data:\")\n",
        "print(dataset['train'][0])\n",
        "\n",
        "# Save dataset locally for faster access\n",
        "data_dir = Path(\"data\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save train/val/test splits\n",
        "dataset['train'].save_to_disk(data_dir / \"train\")\n",
        "dataset['validation'].save_to_disk(data_dir / \"validation\")\n",
        "dataset['test'].save_to_disk(data_dir / \"test\")\n",
        "\n",
        "print(\"\\nDataset saved to local directory!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Initialization\n",
        "\n",
        "Initialize the C2S-Scale-Gemma hybrid model components:\n",
        "- HGNN Encoder with hyperbolic geometry\n",
        "- Gemma text encoder with LoRA adapters\n",
        "- Fusion head for combining representations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import model components\n",
        "import sys\n",
        "sys.path.append('src')\n",
        "\n",
        "from hgnn.encoder import UHGHGNNEncoder\n",
        "from text.gemma_loader import GemmaLoader\n",
        "from text.adapters import LoRAAdapter\n",
        "from fusion.heads import FusionHead\n",
        "from fusion.trainer import DualEncoderTrainer\n",
        "from fusion.align_losses import InfoNCELoss\n",
        "\n",
        "# Model configuration\n",
        "config = {\n",
        "    'model': {\n",
        "        'hgnn': {\n",
        "            'input_dim': 2000,  # Number of highly variable genes\n",
        "            'hidden_dim': 512,\n",
        "            'output_dim': 256,\n",
        "            'num_layers': 3,\n",
        "            'dropout': 0.1,\n",
        "            'curvature': -1.0\n",
        "        },\n",
        "        'text': {\n",
        "            'model_name': 'google/gemma-2-2b',  # Use 2B model for Colab\n",
        "            'max_length': 512,\n",
        "            'hidden_size': 2048,\n",
        "            'quantization': {\n",
        "                'load_in_4bit': True,\n",
        "                'bnb_4bit_compute_dtype': torch.bfloat16,\n",
        "                'bnb_4bit_use_double_quant': True,\n",
        "                'bnb_4bit_quant_type': 'nf4'\n",
        "            },\n",
        "            'lora': {\n",
        "                'r': 16,\n",
        "                'alpha': 32,\n",
        "                'dropout': 0.1,\n",
        "                'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
        "            }\n",
        "        },\n",
        "        'fusion': {\n",
        "            'dim': 512,\n",
        "            'dropout': 0.1\n",
        "        }\n",
        "    },\n",
        "    'training': {\n",
        "        'batch_size': 8,  # Small batch size for Colab\n",
        "        'learning_rate': 1e-4,\n",
        "        'num_epochs': 5,  # Reduced for demo\n",
        "        'contrastive_temperature': 0.07,\n",
        "        'hard_negative_weight': 0.5,\n",
        "        'grad_clip_norm': 1.0\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize HGNN encoder\n",
        "hgnn_encoder = UHGHGNNEncoder(\n",
        "    input_dim=config['model']['hgnn']['input_dim'],\n",
        "    hidden_dim=config['model']['hgnn']['hidden_dim'],\n",
        "    output_dim=config['model']['hgnn']['output_dim'],\n",
        "    num_layers=config['model']['hgnn']['num_layers'],\n",
        "    dropout=config['model']['hgnn']['dropout'],\n",
        "    curvature=config['model']['hgnn']['curvature'],\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"HGNN encoder initialized: {sum(p.numel() for p in hgnn_encoder.parameters())} parameters\")\n",
        "\n",
        "# Initialize Gemma loader\n",
        "gemma_loader = GemmaLoader(\n",
        "    model_name=config['model']['text']['model_name'],\n",
        "    device=device,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=config['model']['text']['quantization']\n",
        ")\n",
        "\n",
        "# Load Gemma model and tokenizer\n",
        "gemma_model, tokenizer = gemma_loader.load_model()\n",
        "print(f\"Gemma model loaded: {sum(p.numel() for p in gemma_model.parameters())} parameters\")\n",
        "\n",
        "# Initialize LoRA adapter\n",
        "lora_adapter = LoRAAdapter(\n",
        "    model=gemma_model,\n",
        "    r=config['model']['text']['lora']['r'],\n",
        "    lora_alpha=config['model']['text']['lora']['alpha'],\n",
        "    lora_dropout=config['model']['text']['lora']['dropout'],\n",
        "    target_modules=config['model']['text']['lora']['target_modules']\n",
        ")\n",
        "\n",
        "print(f\"LoRA adapter initialized: {sum(p.numel() for p in lora_adapter.parameters())} parameters\")\n",
        "\n",
        "# Initialize fusion head\n",
        "fusion_head = FusionHead(\n",
        "    graph_dim=config['model']['hgnn']['output_dim'],\n",
        "    text_dim=config['model']['text']['hidden_size'],\n",
        "    fusion_dim=config['model']['fusion']['dim'],\n",
        "    dropout=config['model']['fusion']['dropout']\n",
        ")\n",
        "\n",
        "print(f\"Fusion head initialized: {sum(p.numel() for p in fusion_head.parameters())} parameters\")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = DualEncoderTrainer(\n",
        "    hgnn_encoder=hgnn_encoder,\n",
        "    text_model=lora_adapter,\n",
        "    fusion_head=fusion_head,\n",
        "    contrastive_loss=InfoNCELoss(\n",
        "        temperature=config['training']['contrastive_temperature'],\n",
        "        hard_negative_weight=config['training']['hard_negative_weight']\n",
        "    ),\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"Total trainable parameters: {sum(p.numel() for p in trainer.parameters() if p.requires_grad)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "\n",
        "Train the hybrid model using contrastive learning to align graph and text representations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training setup\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm import tqdm\n",
        "import mlflow\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = AdamW(\n",
        "    trainer.parameters(),\n",
        "    lr=config['training']['learning_rate'],\n",
        "    weight_decay=0.01,\n",
        "    betas=(0.9, 0.999)\n",
        ")\n",
        "\n",
        "# Initialize scheduler\n",
        "total_steps = len(train_loader) * config['training']['num_epochs']\n",
        "scheduler = CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=total_steps,\n",
        "    eta_min=1e-6\n",
        ")\n",
        "\n",
        "print(f\"Total training steps: {total_steps}\")\n",
        "print(f\"Initial learning rate: {config['training']['learning_rate']}\")\n",
        "\n",
        "# Training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(config['training']['num_epochs']):\n",
        "    # Training\n",
        "    trainer.train()\n",
        "    epoch_train_loss = 0.0\n",
        "    \n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['training']['num_epochs']}\")\n",
        "    \n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Forward pass\n",
        "        loss_dict = trainer.compute_loss(batch)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict['total_loss'].backward()\n",
        "        \n",
        "        # Gradient clipping\n",
        "        if config['training']['grad_clip_norm'] > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                trainer.parameters(), \n",
        "                config['training']['grad_clip_norm']\n",
        "            )\n",
        "        \n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Update metrics\n",
        "        epoch_train_loss += loss_dict['total_loss'].item()\n",
        "        \n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f\"{loss_dict['total_loss'].item():.4f}\",\n",
        "            'contrastive': f\"{loss_dict['contrastive_loss'].item():.4f}\",\n",
        "            'fusion': f\"{loss_dict['fusion_loss'].item():.4f}\",\n",
        "            'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
        "        })\n",
        "    \n",
        "    # Validation\n",
        "    trainer.eval()\n",
        "    epoch_val_loss = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            loss_dict = trainer.compute_loss(batch)\n",
        "            epoch_val_loss += loss_dict['total_loss'].item()\n",
        "    \n",
        "    # Calculate average losses\n",
        "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "    \n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    \n",
        "    print(f\"\\nEpoch {epoch+1}:\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation and Visualization\n",
        "\n",
        "Evaluate the trained model and visualize the learned representations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model and create visualizations\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "\n",
        "# Extract representations\n",
        "trainer.eval()\n",
        "graph_reprs = []\n",
        "text_reprs = []\n",
        "fused_reprs = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"Extracting representations\"):\n",
        "        # Get representations\n",
        "        graph_repr = trainer.get_graph_representation(batch)\n",
        "        text_repr = trainer.get_text_representation(batch)\n",
        "        fused_repr = trainer.get_fused_representation(batch)\n",
        "        \n",
        "        graph_reprs.append(graph_repr.cpu().numpy())\n",
        "        text_reprs.append(text_repr.cpu().numpy())\n",
        "        fused_reprs.append(fused_repr.cpu().numpy())\n",
        "        labels.append(batch['labels'].cpu().numpy())\n",
        "\n",
        "# Concatenate representations\n",
        "graph_reprs = np.concatenate(graph_reprs, axis=0)\n",
        "text_reprs = np.concatenate(text_reprs, axis=0)\n",
        "fused_reprs = np.concatenate(fused_reprs, axis=0)\n",
        "labels = np.concatenate(labels, axis=0)\n",
        "\n",
        "print(f\"Graph representations shape: {graph_reprs.shape}\")\n",
        "print(f\"Text representations shape: {text_reprs.shape}\")\n",
        "print(f\"Fused representations shape: {fused_reprs.shape}\")\n",
        "\n",
        "# Evaluate clustering\n",
        "n_clusters = len(np.unique(labels))\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(fused_reprs)\n",
        "\n",
        "ari = adjusted_rand_score(labels, cluster_labels)\n",
        "nmi = normalized_mutual_info_score(labels, cluster_labels)\n",
        "\n",
        "print(f\"\\nClustering Results:\")\n",
        "print(f\"  Adjusted Rand Index: {ari:.4f}\")\n",
        "print(f\"  Normalized Mutual Information: {nmi:.4f}\")\n",
        "\n",
        "# Create t-SNE visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Graph representations\n",
        "graph_tsne = TSNE(n_components=2, random_state=42).fit_transform(graph_reprs)\n",
        "axes[0, 0].scatter(graph_tsne[:, 0], graph_tsne[:, 1], c=labels, cmap='tab10', alpha=0.7, s=20)\n",
        "axes[0, 0].set_title('Graph Representations')\n",
        "axes[0, 0].set_xlabel('t-SNE 1')\n",
        "axes[0, 0].set_ylabel('t-SNE 2')\n",
        "\n",
        "# Text representations\n",
        "text_tsne = TSNE(n_components=2, random_state=42).fit_transform(text_reprs)\n",
        "axes[0, 1].scatter(text_tsne[:, 0], text_tsne[:, 1], c=labels, cmap='tab10', alpha=0.7, s=20)\n",
        "axes[0, 1].set_title('Text Representations')\n",
        "axes[0, 1].set_xlabel('t-SNE 1')\n",
        "axes[0, 1].set_ylabel('t-SNE 2')\n",
        "\n",
        "# Fused representations\n",
        "fused_tsne = TSNE(n_components=2, random_state=42).fit_transform(fused_reprs)\n",
        "axes[1, 0].scatter(fused_tsne[:, 0], fused_tsne[:, 1], c=labels, cmap='tab10', alpha=0.7, s=20)\n",
        "axes[1, 0].set_title('Fused Representations')\n",
        "axes[1, 0].set_xlabel('t-SNE 1')\n",
        "axes[1, 0].set_ylabel('t-SNE 2')\n",
        "\n",
        "# Training curves\n",
        "axes[1, 1].plot(train_losses, label='Train Loss', color='blue')\n",
        "axes[1, 1].plot(val_losses, label='Validation Loss', color='red')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Loss')\n",
        "axes[1, 1].set_title('Training Progress')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nC2S-Scale-Gemma Hybrid Model Training Complete!\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
